{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T15:27:20.890311Z",
     "start_time": "2019-08-01T15:27:20.884031Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% bash\n",
    "wget https://dumps.wikimedia.org/eswiki/latest/eswiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T20:50:47.509123Z",
     "start_time": "2019-07-30T19:50:01.532307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 articles\n",
      "Processed 20000 articles\n",
      "Processed 30000 articles\n",
      "Processed 40000 articles\n",
      "Processed 50000 articles\n",
      "Processed 60000 articles\n",
      "Processed 70000 articles\n",
      "Processed 80000 articles\n",
      "Processed 90000 articles\n",
      "Processed 100000 articles\n",
      "Processed 110000 articles\n",
      "Processed 120000 articles\n",
      "Processed 130000 articles\n",
      "Processed 140000 articles\n",
      "Processed 150000 articles\n",
      "Processed 160000 articles\n",
      "Processed 170000 articles\n",
      "Processed 180000 articles\n",
      "Processed 190000 articles\n",
      "Processed 200000 articles\n",
      "Processed 210000 articles\n",
      "Processed 220000 articles\n",
      "Processed 230000 articles\n",
      "Processed 240000 articles\n",
      "Processed 250000 articles\n",
      "Processed 260000 articles\n",
      "Processed 270000 articles\n",
      "Processed 280000 articles\n",
      "Processed 290000 articles\n",
      "Processed 300000 articles\n",
      "Processed 310000 articles\n",
      "Processed 320000 articles\n",
      "Processed 330000 articles\n",
      "Processed 340000 articles\n",
      "Processed 350000 articles\n",
      "Processed 360000 articles\n",
      "Processed 370000 articles\n",
      "Processed 380000 articles\n",
      "Processed 390000 articles\n",
      "Processed 400000 articles\n",
      "Processed 410000 articles\n",
      "Processed 420000 articles\n",
      "Processed 430000 articles\n",
      "Processed 440000 articles\n",
      "Processed 450000 articles\n",
      "Processed 460000 articles\n",
      "Processed 470000 articles\n",
      "Processed 480000 articles\n",
      "Processed 490000 articles\n",
      "Processed 500000 articles\n",
      "Processed 510000 articles\n",
      "Processed 520000 articles\n",
      "Processed 530000 articles\n",
      "Processed 540000 articles\n",
      "Processed 550000 articles\n",
      "Processed 560000 articles\n",
      "Processed 570000 articles\n",
      "Processed 580000 articles\n",
      "Processed 590000 articles\n",
      "Processed 600000 articles\n",
      "Processed 610000 articles\n",
      "Processed 620000 articles\n",
      "Processed 630000 articles\n",
      "Processed 640000 articles\n",
      "Processed 650000 articles\n",
      "Processed 660000 articles\n",
      "Processed 670000 articles\n",
      "Processed 680000 articles\n",
      "Processed 690000 articles\n",
      "Processed 700000 articles\n",
      "Processed 710000 articles\n",
      "Processed 720000 articles\n",
      "Processed 730000 articles\n",
      "Processed 740000 articles\n",
      "Processed 750000 articles\n",
      "Processed 760000 articles\n",
      "Processed 770000 articles\n",
      "Processed 780000 articles\n",
      "Processed 790000 articles\n",
      "Processed 800000 articles\n",
      "Processed 810000 articles\n",
      "Processed 820000 articles\n",
      "Processed 830000 articles\n",
      "Processed 840000 articles\n",
      "Processed 850000 articles\n",
      "Processed 860000 articles\n",
      "Processed 870000 articles\n",
      "Processed 880000 articles\n",
      "Processed 890000 articles\n",
      "Processed 900000 articles\n",
      "Processed 910000 articles\n",
      "Processed 920000 articles\n",
      "Processed 930000 articles\n",
      "Processed 940000 articles\n",
      "Processed 950000 articles\n",
      "Processed 960000 articles\n",
      "Processed 970000 articles\n",
      "Processed 980000 articles\n",
      "Processed 990000 articles\n",
      "Processed 1000000 articles\n",
      "Processed 1010000 articles\n",
      "Processed 1020000 articles\n",
      "Processed 1030000 articles\n",
      "Processed 1040000 articles\n",
      "Processed 1050000 articles\n",
      "Processed 1060000 articles\n",
      "Processed 1070000 articles\n",
      "Processed 1080000 articles\n",
      "Processed 1090000 articles\n",
      "Processed 1100000 articles\n",
      "Processed 1110000 articles\n",
      "Processed 1120000 articles\n",
      "Processed 1130000 articles\n",
      "Processed 1140000 articles\n",
      "Processed 1150000 articles\n",
      "Processed 1160000 articles\n",
      "Processed 1170000 articles\n",
      "Processed 1180000 articles\n",
      "Processed 1190000 articles\n",
      "Processed 1200000 articles\n",
      "Processed 1210000 articles\n",
      "Processed 1220000 articles\n",
      "Processed 1230000 articles\n",
      "Processed 1240000 articles\n",
      "Processed 1250000 articles\n",
      "Processed 1260000 articles\n",
      "Processed 1270000 articles\n",
      "Processed 1280000 articles\n",
      "Processed 1290000 articles\n",
      "Processed 1300000 articles\n",
      "Processed 1310000 articles\n",
      "Processed 1320000 articles\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "def make_corpus(in_f, out_f):\n",
    "\n",
    "    \"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n",
    "\n",
    "    output = open(out_f, 'w')\n",
    "    wiki = WikiCorpus(in_f)\n",
    "\n",
    "    i = 0\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            print('Processed ' + str(i) + ' articles')\n",
    "    output.close()\n",
    "    print('Processing complete!')\n",
    "    \n",
    "make_corpus('eswiki-latest-pages-articles.xml.bz2', 'gensim_wiki_corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T15:10:52.454364Z",
     "start_time": "2019-08-01T15:10:43.432892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325626"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "file_len('gensim_wiki_corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T20:34:35.891679Z",
     "start_time": "2019-08-01T15:56:56.469329Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articules 1325626\n",
      "Training batch 1 for the 10000 articles\n",
      "Training batch 2 for the 20000 articles\n",
      "Training batch 3 for the 30000 articles\n",
      "Training batch 4 for the 40000 articles\n",
      "Training batch 5 for the 50000 articles\n",
      "Training batch 6 for the 60000 articles\n",
      "Training batch 7 for the 70000 articles\n",
      "Training batch 8 for the 80000 articles\n",
      "Training batch 9 for the 90000 articles\n",
      "Training batch 10 for the 100000 articles\n",
      "Training batch 11 for the 110000 articles\n",
      "Training batch 12 for the 120000 articles\n",
      "Training batch 13 for the 130000 articles\n",
      "Training batch 14 for the 140000 articles\n",
      "Training batch 15 for the 150000 articles\n",
      "Training batch 16 for the 160000 articles\n",
      "Training batch 17 for the 170000 articles\n",
      "Training batch 18 for the 180000 articles\n",
      "Training batch 19 for the 190000 articles\n",
      "Training batch 20 for the 200000 articles\n",
      "Training batch 21 for the 210000 articles\n",
      "Training batch 22 for the 220000 articles\n",
      "Training batch 23 for the 230000 articles\n",
      "Training batch 24 for the 240000 articles\n",
      "Training batch 25 for the 250000 articles\n",
      "Training batch 26 for the 260000 articles\n",
      "Training batch 27 for the 270000 articles\n",
      "Training batch 28 for the 280000 articles\n",
      "Training batch 29 for the 290000 articles\n",
      "Training batch 30 for the 300000 articles\n",
      "Training batch 31 for the 310000 articles\n",
      "Training batch 32 for the 320000 articles\n",
      "Training batch 33 for the 330000 articles\n",
      "Training batch 34 for the 340000 articles\n",
      "Training batch 35 for the 350000 articles\n",
      "Training batch 36 for the 360000 articles\n",
      "Training batch 37 for the 370000 articles\n",
      "Training batch 38 for the 380000 articles\n",
      "Training batch 39 for the 390000 articles\n",
      "Training batch 40 for the 400000 articles\n",
      "Training batch 41 for the 410000 articles\n",
      "Training batch 42 for the 420000 articles\n",
      "Training batch 43 for the 430000 articles\n",
      "Training batch 44 for the 440000 articles\n",
      "Training batch 45 for the 450000 articles\n",
      "Training batch 46 for the 460000 articles\n",
      "Training batch 47 for the 470000 articles\n",
      "Training batch 48 for the 480000 articles\n",
      "Training batch 49 for the 490000 articles\n",
      "Training batch 50 for the 500000 articles\n",
      "Training batch 51 for the 510000 articles\n",
      "Training batch 52 for the 520000 articles\n",
      "Training batch 53 for the 530000 articles\n",
      "Training batch 54 for the 540000 articles\n",
      "Training batch 55 for the 550000 articles\n",
      "Training batch 56 for the 560000 articles\n",
      "Training batch 57 for the 570000 articles\n",
      "Training batch 58 for the 580000 articles\n",
      "Training batch 59 for the 590000 articles\n",
      "Training batch 60 for the 600000 articles\n",
      "Training batch 61 for the 610000 articles\n",
      "Training batch 62 for the 620000 articles\n",
      "Training batch 63 for the 630000 articles\n",
      "Training batch 64 for the 640000 articles\n",
      "Training batch 65 for the 650000 articles\n",
      "Training batch 66 for the 660000 articles\n",
      "Training batch 67 for the 670000 articles\n",
      "Training batch 68 for the 680000 articles\n",
      "Training batch 69 for the 690000 articles\n",
      "Training batch 70 for the 700000 articles\n",
      "Training batch 71 for the 710000 articles\n",
      "Training batch 72 for the 720000 articles\n",
      "Training batch 73 for the 730000 articles\n",
      "Training batch 74 for the 740000 articles\n",
      "Training batch 75 for the 750000 articles\n",
      "Training batch 76 for the 760000 articles\n",
      "Training batch 77 for the 770000 articles\n",
      "Training batch 78 for the 780000 articles\n",
      "Training batch 79 for the 790000 articles\n",
      "Training batch 80 for the 800000 articles\n",
      "Training batch 81 for the 810000 articles\n",
      "Training batch 82 for the 820000 articles\n",
      "Training batch 83 for the 830000 articles\n",
      "Training batch 84 for the 840000 articles\n",
      "Training batch 85 for the 850000 articles\n",
      "Training batch 86 for the 860000 articles\n",
      "Training batch 87 for the 870000 articles\n",
      "Training batch 88 for the 880000 articles\n",
      "Training batch 89 for the 890000 articles\n",
      "Training batch 90 for the 900000 articles\n",
      "Training batch 91 for the 910000 articles\n",
      "Training batch 92 for the 920000 articles\n",
      "Training batch 93 for the 930000 articles\n",
      "Training batch 94 for the 940000 articles\n",
      "Training batch 95 for the 950000 articles\n",
      "Training batch 96 for the 960000 articles\n",
      "Training batch 97 for the 970000 articles\n",
      "Training batch 98 for the 980000 articles\n",
      "Training batch 99 for the 990000 articles\n",
      "Training batch 100 for the 1000000 articles\n",
      "Training batch 101 for the 1010000 articles\n",
      "Training batch 102 for the 1020000 articles\n",
      "Training batch 103 for the 1030000 articles\n",
      "Training batch 104 for the 1040000 articles\n",
      "Training batch 105 for the 1050000 articles\n",
      "Training batch 106 for the 1060000 articles\n",
      "Training batch 107 for the 1070000 articles\n",
      "Training batch 108 for the 1080000 articles\n",
      "Training batch 109 for the 1090000 articles\n",
      "Training batch 110 for the 1100000 articles\n",
      "Training batch 111 for the 1110000 articles\n",
      "Training batch 112 for the 1120000 articles\n",
      "Training batch 113 for the 1130000 articles\n",
      "Training batch 114 for the 1140000 articles\n",
      "Training batch 115 for the 1150000 articles\n",
      "Training batch 116 for the 1160000 articles\n",
      "Training batch 117 for the 1170000 articles\n",
      "Training batch 118 for the 1180000 articles\n",
      "Training batch 119 for the 1190000 articles\n",
      "Training batch 120 for the 1200000 articles\n",
      "Training batch 121 for the 1210000 articles\n",
      "Training batch 122 for the 1220000 articles\n",
      "Training batch 123 for the 1230000 articles\n",
      "Training batch 124 for the 1240000 articles\n",
      "Training batch 125 for the 1250000 articles\n",
      "Training batch 126 for the 1260000 articles\n",
      "Training batch 127 for the 1270000 articles\n",
      "Training batch 128 for the 1280000 articles\n",
      "Training batch 129 for the 1290000 articles\n",
      "Training batch 130 for the 1300000 articles\n",
      "Training batch 131 for the 1310000 articles\n",
      "Training batch 132 for the 1320000 articles\n",
      "Training batch 133 for the 1330000 articles\n"
     ]
    }
   ],
   "source": [
    "def get_batch(n, file, length):\n",
    "    with open(file) as f:\n",
    "        batch = []\n",
    "        for i, l in enumerate(f):\n",
    "            batch += [l.split()]\n",
    "            if (i+1)%n==0 or i == length - 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(size=300, workers=4)\n",
    "data_length = file_len('gensim_wiki_corpora')\n",
    "batch_len = 10000\n",
    "print(\"Total number of articules {}\".format(data_length))\n",
    "for i, batch in enumerate(get_batch(batch_len, 'gensim_wiki_corpora', data_length)):\n",
    "    print(\"Training batch {} for the {} articles\".format(i+1, (i+1)*batch_len))\n",
    "    if i:\n",
    "        model.build_vocab(batch, update=True)\n",
    "    else:\n",
    "        model.build_vocab(batch)\n",
    "        \n",
    "    model.train(batch, total_examples=len(batch), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T20:48:24.993402Z",
     "start_time": "2019-08-01T20:48:13.688185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/anaconda3/envs/nlp/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model/basefft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T20:44:52.548662Z",
     "start_time": "2019-08-01T20:44:52.540867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517213"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "14",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
