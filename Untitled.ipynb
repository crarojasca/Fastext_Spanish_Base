{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T15:27:20.890311Z",
     "start_time": "2019-08-01T15:27:20.884031Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% bash\n",
    "wget https://dumps.wikimedia.org/eswiki/latest/eswiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T20:50:47.509123Z",
     "start_time": "2019-07-30T19:50:01.532307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 articles\n",
      "Processed 20000 articles\n",
      "Processed 30000 articles\n",
      "Processed 40000 articles\n",
      "Processed 50000 articles\n",
      "Processed 60000 articles\n",
      "Processed 70000 articles\n",
      "Processed 80000 articles\n",
      "Processed 90000 articles\n",
      "Processed 100000 articles\n",
      "Processed 110000 articles\n",
      "Processed 120000 articles\n",
      "Processed 130000 articles\n",
      "Processed 140000 articles\n",
      "Processed 150000 articles\n",
      "Processed 160000 articles\n",
      "Processed 170000 articles\n",
      "Processed 180000 articles\n",
      "Processed 190000 articles\n",
      "Processed 200000 articles\n",
      "Processed 210000 articles\n",
      "Processed 220000 articles\n",
      "Processed 230000 articles\n",
      "Processed 240000 articles\n",
      "Processed 250000 articles\n",
      "Processed 260000 articles\n",
      "Processed 270000 articles\n",
      "Processed 280000 articles\n",
      "Processed 290000 articles\n",
      "Processed 300000 articles\n",
      "Processed 310000 articles\n",
      "Processed 320000 articles\n",
      "Processed 330000 articles\n",
      "Processed 340000 articles\n",
      "Processed 350000 articles\n",
      "Processed 360000 articles\n",
      "Processed 370000 articles\n",
      "Processed 380000 articles\n",
      "Processed 390000 articles\n",
      "Processed 400000 articles\n",
      "Processed 410000 articles\n",
      "Processed 420000 articles\n",
      "Processed 430000 articles\n",
      "Processed 440000 articles\n",
      "Processed 450000 articles\n",
      "Processed 460000 articles\n",
      "Processed 470000 articles\n",
      "Processed 480000 articles\n",
      "Processed 490000 articles\n",
      "Processed 500000 articles\n",
      "Processed 510000 articles\n",
      "Processed 520000 articles\n",
      "Processed 530000 articles\n",
      "Processed 540000 articles\n",
      "Processed 550000 articles\n",
      "Processed 560000 articles\n",
      "Processed 570000 articles\n",
      "Processed 580000 articles\n",
      "Processed 590000 articles\n",
      "Processed 600000 articles\n",
      "Processed 610000 articles\n",
      "Processed 620000 articles\n",
      "Processed 630000 articles\n",
      "Processed 640000 articles\n",
      "Processed 650000 articles\n",
      "Processed 660000 articles\n",
      "Processed 670000 articles\n",
      "Processed 680000 articles\n",
      "Processed 690000 articles\n",
      "Processed 700000 articles\n",
      "Processed 710000 articles\n",
      "Processed 720000 articles\n",
      "Processed 730000 articles\n",
      "Processed 740000 articles\n",
      "Processed 750000 articles\n",
      "Processed 760000 articles\n",
      "Processed 770000 articles\n",
      "Processed 780000 articles\n",
      "Processed 790000 articles\n",
      "Processed 800000 articles\n",
      "Processed 810000 articles\n",
      "Processed 820000 articles\n",
      "Processed 830000 articles\n",
      "Processed 840000 articles\n",
      "Processed 850000 articles\n",
      "Processed 860000 articles\n",
      "Processed 870000 articles\n",
      "Processed 880000 articles\n",
      "Processed 890000 articles\n",
      "Processed 900000 articles\n",
      "Processed 910000 articles\n",
      "Processed 920000 articles\n",
      "Processed 930000 articles\n",
      "Processed 940000 articles\n",
      "Processed 950000 articles\n",
      "Processed 960000 articles\n",
      "Processed 970000 articles\n",
      "Processed 980000 articles\n",
      "Processed 990000 articles\n",
      "Processed 1000000 articles\n",
      "Processed 1010000 articles\n",
      "Processed 1020000 articles\n",
      "Processed 1030000 articles\n",
      "Processed 1040000 articles\n",
      "Processed 1050000 articles\n",
      "Processed 1060000 articles\n",
      "Processed 1070000 articles\n",
      "Processed 1080000 articles\n",
      "Processed 1090000 articles\n",
      "Processed 1100000 articles\n",
      "Processed 1110000 articles\n",
      "Processed 1120000 articles\n",
      "Processed 1130000 articles\n",
      "Processed 1140000 articles\n",
      "Processed 1150000 articles\n",
      "Processed 1160000 articles\n",
      "Processed 1170000 articles\n",
      "Processed 1180000 articles\n",
      "Processed 1190000 articles\n",
      "Processed 1200000 articles\n",
      "Processed 1210000 articles\n",
      "Processed 1220000 articles\n",
      "Processed 1230000 articles\n",
      "Processed 1240000 articles\n",
      "Processed 1250000 articles\n",
      "Processed 1260000 articles\n",
      "Processed 1270000 articles\n",
      "Processed 1280000 articles\n",
      "Processed 1290000 articles\n",
      "Processed 1300000 articles\n",
      "Processed 1310000 articles\n",
      "Processed 1320000 articles\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "def make_corpus(in_f, out_f):\n",
    "\n",
    "    \"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n",
    "\n",
    "    output = open(out_f, 'w')\n",
    "    wiki = WikiCorpus(in_f)\n",
    "\n",
    "    i = 0\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            print('Processed ' + str(i) + ' articles')\n",
    "    output.close()\n",
    "    print('Processing complete!')\n",
    "    \n",
    "make_corpus('eswiki-latest-pages-articles.xml.bz2', 'gensim_wiki_corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T15:10:52.454364Z",
     "start_time": "2019-08-01T15:10:43.432892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325626"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "file_len('gensim_wiki_corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T15:56:56.463Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articules 1325626\n",
      "Training batch 1 for the 10000 articles\n",
      "Training batch 2 for the 20000 articles\n",
      "Training batch 3 for the 30000 articles\n",
      "Training batch 4 for the 40000 articles\n",
      "Training batch 5 for the 50000 articles\n",
      "Training batch 6 for the 60000 articles\n",
      "Training batch 7 for the 70000 articles\n"
     ]
    }
   ],
   "source": [
    "def get_batch(n, file, length):\n",
    "    with open(file) as f:\n",
    "        batch = []\n",
    "        for i, l in enumerate(f):\n",
    "            batch += [l.split()]\n",
    "            if (i+1)%n==0 or i == length - 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(size=300, workers=4)\n",
    "data_length = file_len('gensim_wiki_corpora')\n",
    "batch_len = 10000\n",
    "print(\"Total number of articules {}\".format(data_length))\n",
    "for i, batch in enumerate(get_batch(batch_len, 'gensim_wiki_corpora', data_length)):\n",
    "    print(\"Training batch {} for the {} articles\".format(i+1, (i+1)*batch_len))\n",
    "    if i:\n",
    "        model.build_vocab(batch, update=True)\n",
    "    else:\n",
    "        model.build_vocab(batch)\n",
    "        \n",
    "    model.train(batch, total_examples=len(batch), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T15:19:56.486870Z",
     "start_time": "2019-08-01T15:19:51.128337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.5626"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_length = file_len('gensim_wiki_corpora')\n",
    "data_length/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T15:20:15.535603Z",
     "start_time": "2019-08-01T15:20:15.527834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_length//10000 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "14",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
